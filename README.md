.

ğŸ“˜ Linear Regression â€“ Complete Professional Notes
ğŸ”¹ 1. What is Linear Regression?

Linear Regression is a supervised machine learning algorithm used to predict continuous numerical values.

It models the relationship between independent variables (X) and dependent variable (Y) by fitting a straight line.

It assumes a linear relationship between input features and output.

Common use cases:

House price prediction

Salary prediction

Sales forecasting

ğŸ”¹ 2. Mathematical Model

For single feature:

ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘¥
y=b
0
	â€‹

+b
1
	â€‹

x

For multiple features:

ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘¥
1
+
ğ‘
2
ğ‘¥
2
+
.
.
.
+
ğ‘
ğ‘›
ğ‘¥
ğ‘›
y=b
0
	â€‹

+b
1
	â€‹

x
1
	â€‹

+b
2
	â€‹

x
2
	â€‹

+...+b
n
	â€‹

x
n
	â€‹


Where:

bâ‚€ = Intercept

bâ‚ = Slope

y = Predicted value

ğŸ”¹ 3. Line of Best Fit (Concept)

The model finds the best straight line that minimizes prediction error.

Y
|
|                    â—
|               â—
|          â—
|     â—
| â—
|________________________________ X
         \
          \
           \  Best Fit Line

ğŸ”¹ 4. Residuals

Residual = Actual âˆ’ Predicted

ğ‘…
ğ‘’
ğ‘ 
ğ‘–
ğ‘‘
ğ‘¢
ğ‘
ğ‘™
=
ğ‘¦
âˆ’
ğ‘¦
^
Residual=yâˆ’
y
^
	â€‹


Residual is vertical distance between data point and regression line.

Good Model:
Residuals randomly scattered around zero.

Bad Model:
Residuals show pattern â†’ non-linear relationship.

ğŸ”¹ 5. Cost Function (MSE)
ğ‘€
ğ‘†
ğ¸
=
1
ğ‘›
âˆ‘
(
ğ‘¦
âˆ’
ğ‘¦
^
)
2
MSE=
n
1
	â€‹

âˆ‘(yâˆ’
y
^
	â€‹

)
2

The goal of Linear Regression is to minimize MSE.

Lower MSE = Better model.

ğŸ”¹ 6. Gradient Descent

Used to minimize cost function.

Update Rule:

ğ‘
=
ğ‘
âˆ’
ğ›¼
Ã—
ğ‘”
ğ‘Ÿ
ğ‘
ğ‘‘
ğ‘–
ğ‘’
ğ‘›
ğ‘¡
b=bâˆ’Î±Ã—gradient

Learning Rate (Î±):

Too small â†’ Slow training

Too large â†’ Overshoot

Cost decreases gradually until convergence.

ğŸ”¹ 7. Evaluation Metrics

MAE â€“ Mean Absolute Error
MSE â€“ Mean Squared Error
RMSE â€“ Root Mean Squared Error
RÂ² â€“ Variance explained by model

ğ‘…
2
=
1
âˆ’
ğ‘†
ğ‘†
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘†
ğ‘†
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
R
2
=1âˆ’
SS
total
	â€‹

SS
res
	â€‹

	â€‹

ğŸ”¹ 8. Adjusted RÂ²

RÂ² increases when features are added.

Adjusted RÂ² penalizes unnecessary features.

Used in Multiple Linear Regression.

ğŸ”¹ 9. Underfitting vs Overfitting

Underfitting:
Model too simple â†’ High bias

Overfitting:
Model too complex â†’ High variance

Regularization helps control this.

ğŸ”¹ 10. Ridge Regression (L2)
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
ğ‘
2
MSE+Î»âˆ‘b
2

Shrinks coefficients

Handles multicollinearity

Reduces overfitting

ğŸ”¹ 11. Lasso Regression (L1)
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
âˆ£
ğ‘
âˆ£
MSE+Î»âˆ‘âˆ£bâˆ£

Can make coefficients zero

Performs feature selection

Produces simpler model

ğŸ”¹ 12. Bias-Variance Tradeoff

Underfitting â†’ High Bias
Overfitting â†’ High Variance

Goal â†’ Balance both.
