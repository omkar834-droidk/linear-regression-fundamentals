.

ğŸ“˜ Linear Regression â€“ Complete Structured Notes
1ï¸âƒ£ Introduction

Linear Regression is a supervised learning algorithm used to predict continuous numerical values.
It models the relationship between independent variables (X) and a dependent variable (Y) using a straight line.

It assumes that there is a linear relationship between input features and output.

Common examples:

House price prediction

Salary estimation

Sales forecasting

2ï¸âƒ£ Mathematical Model
Simple Linear Regression
ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘¥
y=b
0
	â€‹

+b
1
	â€‹

x

Where:

y = Predicted value

x = Input feature

bâ‚€ = Intercept

bâ‚ = Slope

Multiple Linear Regression
ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘¥
1
+
ğ‘
2
ğ‘¥
2
+
.
.
.
+
ğ‘
ğ‘›
ğ‘¥
ğ‘›
y=b
0
	â€‹

+b
1
	â€‹

x
1
	â€‹

+b
2
	â€‹

x
2
	â€‹

+...+b
n
	â€‹

x
n
	â€‹


The goal is to find coefficients that minimize error.

3ï¸âƒ£ Line of Best Fit

Linear Regression finds the best straight line that minimizes total prediction error.

        Y
        |
        |                    â—
        |               â—
        |          â—
        |     â—
        | â—
        |________________________________ X
                 \
                  \
                   \  Best Fit Line


The slope determines direction of relationship.

4ï¸âƒ£ Residuals (Error Concept)

Residual is the vertical distance between actual value and predicted value.

ğ‘…
ğ‘’
ğ‘ 
ğ‘–
ğ‘‘
ğ‘¢
ğ‘
ğ‘™
=
ğ‘¦
âˆ’
ğ‘¦
^
Residual=yâˆ’
y
^
	â€‹

           â—  (Actual)
           |
           |   Residual
           |
-----------+------------------
          Regression Line


Good model â†’ Residuals randomly scattered
Bad model â†’ Residuals show pattern

5ï¸âƒ£ Cost Function

Linear Regression minimizes Mean Squared Error (MSE).

ğ‘€
ğ‘†
ğ¸
=
1
ğ‘›
âˆ‘
(
ğ‘¦
âˆ’
ğ‘¦
^
)
2
MSE=
n
1
	â€‹

âˆ‘(yâˆ’
y
^
	â€‹

)
2

Why square errors?

Removes negative sign

Penalizes large errors more

Lower MSE means better model performance.

6ï¸âƒ£ Gradient Descent

Gradient Descent is used to minimize the cost function.

Update rule:

ğ‘
=
ğ‘
âˆ’
ğ›¼
Ã—
âˆ‚
ğ¶
ğ‘œ
ğ‘ 
ğ‘¡
âˆ‚
ğ‘
b=bâˆ’Î±Ã—
âˆ‚b
âˆ‚Cost
	â€‹


Where Î± is learning rate.

Cost
  |
  |\
  | \
  |  \
  |   \
  |    \____
  |
  +---------------- Iterations


Learning rate controls speed of convergence.

7ï¸âƒ£ Model Evaluation Metrics
MAE

Average absolute difference between actual and predicted values.

MSE

Average squared difference.

RMSE

Square root of MSE. Same unit as target.

RÂ² Score
ğ‘…
2
=
1
âˆ’
ğ‘†
ğ‘†
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘†
ğ‘†
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
R
2
=1âˆ’
SS
total
	â€‹

SS
res
	â€‹

	â€‹


Range: 0 to 1
Higher value â†’ Better model

8ï¸âƒ£ Adjusted RÂ²

RÂ² increases when more features are added, even if they are useless.

Adjusted RÂ² penalizes unnecessary features.

ğ´
ğ‘‘
ğ‘—
ğ‘¢
ğ‘ 
ğ‘¡
ğ‘’
ğ‘‘
 
ğ‘…
2
=
1
âˆ’
(
1
âˆ’
ğ‘…
2
)
(
ğ‘›
âˆ’
1
)
(
ğ‘›
âˆ’
ğ‘˜
âˆ’
1
)
Adjusted R
2
=1âˆ’
(nâˆ’kâˆ’1)
(1âˆ’R
2
)(nâˆ’1)
	â€‹


Useful for comparing multiple regression models.

9ï¸âƒ£ Underfitting vs Overfitting
Underfitting

Model too simple

High bias

Poor performance on train & test

Data:   â—   â—   â—
Model:  ----------

Overfitting

Model too complex

High variance

High train accuracy, low test accuracy

Data:   â—   â—   â—
Model:  /\/\/\/\/\/\


Regularization helps control overfitting.

ğŸ”Ÿ Regularization

Regularization adds penalty to large coefficients.

New objective:

Minimize (MSE + Penalty)

1ï¸âƒ£1ï¸âƒ£ Ridge Regression (L2)
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
ğ‘
2
MSE+Î»âˆ‘b
2

Shrinks coefficients

Reduces variance

Handles multicollinearity

1ï¸âƒ£2ï¸âƒ£ Lasso Regression (L1)
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
âˆ£
ğ‘
âˆ£
MSE+Î»âˆ‘âˆ£bâˆ£

Shrinks coefficients

Can make some exactly zero

Performs feature selection

1ï¸âƒ£3ï¸âƒ£ Bias-Variance Tradeoff

Underfitting â†’ High Bias
Overfitting â†’ High Variance

Goal: Balance bias and variance.

Regularization helps achieve that balance.

Final Summary

Linear Regression predicts continuous values using a best-fit line.
Residuals measure prediction error.
Gradient Descent minimizes cost.
RÂ² evaluates model performance.
Adjusted RÂ² prevents misleading feature addition.
Ridge and Lasso prevent overfitting using regularization.
