.

ğŸ“˜ Linear Regression â€“ Complete Structured Notes
1ï¸âƒ£ Introduction

Linear Regression is a supervised learning algorithm used to predict continuous numerical values.
It models the relationship between independent variables (X) and a dependent variable (Y) using a straight line.

It assumes that there is a linear relationship between input features and output.

Common examples:

House price prediction

Salary estimation

Sales forecasting

2ï¸âƒ£ Mathematical Model
Simple Linear Regression
ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘¥
y=b
0
	â€‹

+b
1
	â€‹

x

Where:

y = Predicted value

x = Input feature

bâ‚€ = Intercept

bâ‚ = Slope

Multiple Linear Regression
ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘¥
1
+
ğ‘
2
ğ‘¥
2
+
.
.
.
+
ğ‘
ğ‘›
ğ‘¥
ğ‘›
y=b
0
	â€‹

+b
1
	â€‹

x
1
	â€‹

+b
2
	â€‹

x
2
	â€‹

+...+b
n
	â€‹

x
n
	â€‹


The goal is to find coefficients that minimize error.

3ï¸âƒ£ Line of Best Fit

Linear Regression finds the best straight line that minimizes total prediction error.

        Y
        |
        |                    â—
        |               â—
        |          â—
        |     â—
        | â—
        |________________________________ X
                 \
                  \
                   \  Best Fit Line




	   <img width="3024" height="2160" alt="image" src="https://github.com/user-attachments/assets/b56953bd-b615-4aaa-8ec5-93e98eafdab2" />


The slope determines direction of relationship.

4ï¸âƒ£ Residuals (Error Concept)

Residual is the vertical distance between actual value and predicted value.

ğ‘…
ğ‘’
ğ‘ 
ğ‘–
ğ‘‘
ğ‘¢
ğ‘
ğ‘™
=
ğ‘¦
âˆ’
ğ‘¦
^
Residual=yâˆ’
y
^
	â€‹

           â—  (Actual)
           |
           |   Residual
           |
-----------+------------------
          Regression Line


Good model â†’ Residuals randomly scattered
Bad model â†’ Residuals show pattern

5ï¸âƒ£ Cost Function

Linear Regression minimizes Mean Squared Error (MSE).

ğ‘€
ğ‘†
ğ¸
=
1
ğ‘›
âˆ‘
(
ğ‘¦
âˆ’
ğ‘¦
^
)
2
MSE=
n
1
	â€‹

âˆ‘(yâˆ’
y
^
	â€‹

)
2

Why square errors?

Removes negative sign

Penalizes large errors more

Lower MSE means better model performance.

6ï¸âƒ£ Gradient Descent

Gradient Descent is used to minimize the cost function.

Update rule:

ğ‘
=
ğ‘
âˆ’
ğ›¼
Ã—
âˆ‚
ğ¶
ğ‘œ
ğ‘ 
ğ‘¡
âˆ‚
ğ‘
b=bâˆ’Î±Ã—
âˆ‚b
âˆ‚Cost
	â€‹


Where Î± is learning rate.

Cost
  |
  |\
  | \
  |  \
  |   \
  |    \____
  |
  +---------------- Iterations


Learning rate controls speed of convergence.

7ï¸âƒ£ Model Evaluation Metrics
MAE

Average absolute difference between actual and predicted values.

MSE

Average squared difference.

RMSE

Square root of MSE. Same unit as target.

RÂ² Score
ğ‘…
2
=
1
âˆ’
ğ‘†
ğ‘†
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘†
ğ‘†
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
R
2
=1âˆ’
SS
total
	â€‹

SS
res
	â€‹

	â€‹


Range: 0 to 1
Higher value â†’ Better model

8ï¸âƒ£ Adjusted RÂ²

RÂ² increases when more features are added, even if they are useless.

Adjusted RÂ² penalizes unnecessary features.

ğ´
ğ‘‘
ğ‘—
ğ‘¢
ğ‘ 
ğ‘¡
ğ‘’
ğ‘‘
 
ğ‘…
2
=
1
âˆ’
(
1
âˆ’
ğ‘…
2
)
(
ğ‘›
âˆ’
1
)
(
ğ‘›
âˆ’
ğ‘˜
âˆ’
1
)
Adjusted R
2
=1âˆ’
(nâˆ’kâˆ’1)
(1âˆ’R
2
)(nâˆ’1)
	â€‹


Useful for comparing multiple regression models.

9ï¸âƒ£ Underfitting vs Overfitting
Underfitting

Model too simple

High bias

Poor performance on train & test

Data:   â—   â—   â—
Model:  ----------

Overfitting

Model too complex

High variance

High train accuracy, low test accuracy

Data:   â—   â—   â—
Model:  /\/\/\/\/\/\


Regularization helps control overfitting.

ğŸ”Ÿ Regularization

Regularization adds penalty to large coefficients.

New objective:

Minimize (MSE + Penalty)

1ï¸âƒ£1ï¸âƒ£ Ridge Regression (L2)
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
ğ‘
2
MSE+Î»âˆ‘b
2

Shrinks coefficients

Reduces variance

Handles multicollinearity

1ï¸âƒ£2ï¸âƒ£ Lasso Regression (L1)
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
âˆ£
ğ‘
âˆ£
MSE+Î»âˆ‘âˆ£bâˆ£

Shrinks coefficients

Can make some exactly zero

Performs feature selection

1ï¸âƒ£3ï¸âƒ£ Bias-Variance Tradeoff

Underfitting â†’ High Bias
Overfitting â†’ High Variance

Goal: Balance bias and variance.

Regularization helps achieve that balance.

Final Summary

Linear Regression predicts continuous values using a best-fit line.
Residuals measure prediction error.
Gradient Descent minimizes cost.
RÂ² evaluates model performance.
Adjusted RÂ² prevents misleading feature addition.
Ridge and Lasso prevent overfitting using regularization.

  


# ================================
# Linear Regression Full Pipeline
# Linear vs Ridge vs Lasso
# ================================

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	
	from sklearn.datasets import load_diabetes
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler
	from sklearn.linear_model import LinearRegression, Ridge, Lasso
	from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -------------------------------
# 1. Load Dataset
# -------------------------------

	data = load_diabetes()
	X = pd.DataFrame(data.data, columns=data.feature_names)
	y = data.target

# -------------------------------
# 2. Train-Test Split
# -------------------------------

	X_train, X_test, y_train, y_test = train_test_split(
	    X, y, test_size=0.2, random_state=42
)

# -------------------------------
# 3. Scaling (Important for Ridge & Lasso)
# -------------------------------

	scaler = StandardScaler()
	X_train_scaled = scaler.fit_transform(X_train)
	X_test_scaled = scaler.transform(X_test)

# -------------------------------
# 4. Initialize Models
# -------------------------------

	models = {
	    "Linear Regression": LinearRegression(),
	    "Ridge Regression": Ridge(alpha=1.0),
	    "Lasso Regression": Lasso(alpha=0.1)
	}

# -------------------------------
# 5. Train & Evaluate
# -------------------------------

	for name, model in models.items():
    
    model.fit(X_train_scaled, y_train)
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)
    
    print(f"\n========== {name} ==========")
    
    # Evaluation Metrics
    print("Train R2:", r2_score(y_train, y_pred_train))
    print("Test R2 :", r2_score(y_test, y_pred_test))
    print("MAE     :", mean_absolute_error(y_test, y_pred_test))
    print("MSE     :", mean_squared_error(y_test, y_pred_test))
    print("RMSE    :", np.sqrt(mean_squared_error(y_test, y_pred_test)))
    
    # Adjusted R2
    n = X_test.shape[0]
    k = X_test.shape[1]
    r2 = r2_score(y_test, y_pred_test)
    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))
    print("Adjusted R2:", adj_r2)

# -------------------------------
# 6. Residual Plot (Linear Model)
# -------------------------------

	linear_model = LinearRegression()
	linear_model.fit(X_train_scaled, y_train)
    y_pred = linear_model.predict(X_test_scaled)

	residuals = y_test - y_pred

	plt.figure()
	plt.scatter(y_pred, residuals)
	plt.axhline(y=0)
	plt.xlabel("Predicted Values")
	plt.ylabel("Residuals")
	plt.title("Residual Plot (Linear Regression)")
	plt.show()

